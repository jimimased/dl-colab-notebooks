Try out deep learning models online on Colab with a single click.

## Image Super-Resolution
* Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN). A combination of [xinntao/ESRGAN](https://github.com/xinntao/ESRGAN) and [ata4/esrgan-launcher](https://github.com/ata4/esrgan-launcher). My colab fork is located in [styler00dollar/Colab-ESRGAN](https://github.com/styler00dollar/Colab-ESRGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-ESRGAN/blob/master/Colab-ESRGAN-(new&old-arch).ipynb)
  * Train ESRGAN with forked [xinntao/BasicSR](https://github.com/xinntao/BasicSR) ([victorca25/BasicSR](https://github.com/victorca25/BasicSR)) and my fork [styler00dollar/Colab-BasicSR](https://github.com/styler00dollar/Colab-BasicSR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-BasicSR/blob/master/Colab-BasicSR.ipynb)
* Deep Unfolding Network for Image Super-Resolution (USRNet). The original repositories are [cszn/USRNet](https://github.com/cszn/USRNet) and [cszn/KAIR](https://github.com/cszn/KAIR). My colab fork is located in [styler00dollar/Colab-USRNet](https://github.com/styler00dollar/Colab-USRNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-USRNet/blob/master/Colab-USRNet.ipynb)
* Image Super Resolution with [idealo/image-super-resolution](https://github.com/idealo/image-super-resolution). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/ISR_Prediction_Tutorial.ipynb)
* PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb)
* SPSR: Structure-Preserving Super Resolution with Gradient Guidance with [Maclory/SPSR](https://github.com/Maclory/SPSR). A Colab based on [BlueAmulet/SPSR](https://github.com/BlueAmulet/SPSR) is located in my fork [styler00dollar/Colab-SPSR](https://github.com/styler00dollar/Colab-SPSR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-SPSR/blob/master/Colab-SPSR.ipynb)

## Video Interpolation
* RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation with [hzwer/arXiv2020-RIFE](https://github.com/hzwer/arXiv2020-RIFE). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hzwer/arXiv2020-RIFE/blob/main/Colab_demo.ipynb) My fork with FFMPEG is located in [styler00dollar/Colab-RIFE](https://github.com/styler00dollar/Colab-RIFE). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-RIFE/blob/main/Colab-RIFE-FFMPEG.ipynb)
  * Alternative with [HeylonNHP/RIFE-Colab](https://github.com/HeylonNHP/RIFE-Colab). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HeylonNHP/RIFE-Colab/blob/main/RIFE_Colab.ipynb)
* Depth-Aware Video Frame Interpolation (DAIN) using [baowenbo/DAIN](https://github.com/baowenbo/DAIN). My fork is located in [styler00dollar/Colab-DAIN](https://github.com/styler00dollar/Colab-DAIN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DAIN/blob/master/Colab_DAIN.ipynb)
  * DAIN NCNN with [nihui/dain-ncnn-vulkan](https://github.com/nihui/dain-ncnn-vulkan). My Colab fork is located in [styler00dollar/Colab-dain-ncnn-vulkan](https://github.com/styler00dollar/Colab-dain-ncnn-vulkan). [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-dain-ncnn-vulkan/blob/master/Colab-dain-ncnn-vulkan.ipynb)
* CAIN NCNN (Channel Attention Is All You Need for Video Frame Interpolation) with [nihui/cain-ncnn-vulkan](https://github.com/nihui/cain-ncnn-vulkan). My fork is located in [styler00dollar/Colab-cain-ncnn-vulkan](https://github.com/styler00dollar/Colab-cain-ncnn-vulkan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-cain-ncnn-vulkan/blob/master/Colab-cain-vulkan.ipynb)
* High Quality Estimation of Multiple Intermediate Frames for Video Interpolation with [avinashpaliwal/Super-SloMo](https://github.com/avinashpaliwal/Super-SloMo). My colab fork is locaced in [styler00dollar/Colab-Super-SloMo](https://github.com/styler00dollar/Colab-Super-SloMo). My version: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Super-SloMo/blob/master/Colab-Super-SloMo.ipynb) The original: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SuperSloMo.ipynb)
* Featureflow can be found in [CM-BF/FeatureFlow](https://github.com/CM-BF/FeatureFlow). The colab was made by Mr. Anon. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eg-ApEMzYRGUoacEtnxMWN9lESw_0E1C?usp=sharing)
* Video Frame Interpolation via Residue Refinement with [HopLee6/RRIN](https://github.com/HopLee6/RRIN). My fork is located in [styler00dollar/Colab-RRIN](https://github.com/styler00dollar/Colab-RRIN). [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-RRIN/blob/master/Colab-RRIN.ipynb)

## Video Super-Resolution and Interpolation
* Fast and Accurate One-Stage Space-Time Video Super-Resolution with [Mukosame/Zooming-Slow-Mo-CVPR-2020](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020). My colab fork is located in [styler00dollar/Colab-Zooming-Slow-Mo](https://github.com/styler00dollar/Colab-Zooming-Slow-Mo). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Zooming-Slow-Mo/blob/master/Colab-Zooming-Slow-Mo.ipynb)

## Inpainting
* DFNet: Deep Fusion Network for Image completion with [hughplay/DFNet](https://github.com/hughplay/DFNet). My fork with [Yukariin/DFNet](https://github.com/Yukariin/DFNet) is located in [styler00dollar/Colab-DFNet](https://github.com/styler00dollar/Colab-DFNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DFNet/blob/master/Colab-DFNet.ipynb)
* [open-mmlab/mmediting](https://github.com/open-mmlab/mmediting) is an open source image and video editing toolbox based on PyTorch. My fork is located in [styler00dollar/Colab-mmediting](https://github.com/styler00dollar/Colab-mmediting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-mmediting/blob/master/Colab-mmediting.ipynb)
* EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning with [knazeri/edge-connect](https://github.com/knazeri/edge-connect). My fork is located in [styler00dollar/Colab-edge-connect](https://github.com/styler00dollar/Colab-edge-connect). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-edge-connect/blob/master/Colab-edge-connect.ipynb)
* Rethinking Inpainting with [KumapowerLIU/Rethinking-Inpainting-MEDFE](https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE). My fork is located in [styler00dollar/Colab-MEDFE](https://github.com/styler00dollar/Colab-MEDFE). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-MEDFE/blob/master/Colab-MEDFE.ipynb) **[WARNING: NEEDS MATLAB]**
* Region Normalization for Image Inpainting with [geekyutao/RN](https://github.com/geekyutao/RN). My fork is located in [styler00dollar/Colab-RN](https://github.com/styler00dollar/Colab-RN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-RN/blob/master/Colab-RN.ipynb)
* Coherent Semantic Attention Image Inpainting with [Yukariin/CSA_pytorch](https://github.com/Yukariin/CSA_pytorch). My fork is located in [styler00dollar/Colab-CSA-pytorch](https://github.com/styler00dollar/Colab-CSA-pytorch). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-CSA-pytorch/blob/master/Colab-CSA-pytorch.ipynb)
* Pluralistic Image Completion with [lyndonzheng/Pluralistic-Inpainting](https://github.com/lyndonzheng/Pluralistic-Inpainting). My fork is located in [styler00dollar/Colab-Pluralistic-Inpainting](https://github.com/styler00dollar/Colab-Pluralistic-Inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Pluralistic-Inpainting/blob/master/Colab-Pluralistic-Inpainting.ipynb)
* [SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting](https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting). My fork is located in [styler00dollar/Colab-Global-and-Local-Inpainting](https://github.com/styler00dollar/Colab-Global-and-Local-Inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Global-and-Local-Inpainting/blob/master/Colab-Global-and-Local-Inpainting.ipynb)
* [JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting) aka DeepFill v1/v2 with Contextual Attention and Gated Convolution. My fork is located in [styler00dollar/Colab-generative-inpainting](https://github.com/styler00dollar/Colab-generative-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-generative-inpainting/blob/master/Colab-deepfillv2.ipynb)
* Video inpainting with Flow-edge Guided Video Completion can be found in [vt-vl-lab/FGVC](https://github.com/vt-vl-lab/FGVC). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc?usp=sharing)
  * Alternative with [brunomsantiago/FGVC_video_inpaint_colab_drive](https://github.com/brunomsantiago/FGVC_video_inpaint_colab_drive). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brunomsantiago/FGVC_video_inpaint_colab_drive/blob/main/FGVC_video_completion.ipynb)
* A lightning Colab that is a combination of a lot of different inpainting networks and loss functions. May also support super resolution in the future, but for now, only inpainting generators are the focus. My Colab is located in [styler00dollar/Colab-BasicSR](https://github.com/styler00dollar/Colab-BasicSR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-BasicSR/blob/master/Colab-BasicSR-lightning.ipynb)

## TTS
* LibriTTS trained multi speaker TTS demo using [NVIDIA/flowtron](https://github.com/NVIDIA/flowtron).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidia_Flowtron_Waveglow.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [Rayhane-mamah/Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2) and [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb)
* A Mongolian male voice demo using [Rayhane-mamah/Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2) with the Griffin-Lim algorithm.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/mongolian-nlp/blob/master/misc/Tacotron_MongolianTTS.ipynb)  
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts) with the Griffin-Lim algorithm.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/pytorch-dc-tts/blob/master/notebooks/EnglishTTS.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) (Tacotron + WaveRNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/fatchordWaveRNN.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [mozilla/TTS](https://github.com/mozilla/TTS) (Tacotron + WaveRNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Mozilla_TTS_WaveRNN.ipynb)
* [NVIDIA/mellotron](https://github.com/NVIDIA/mellotron) notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yhgon/mellotron/blob/master/inference_colab.ipynb)
* Voice clone demo using [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/RealTimeVoiceCloning.ipynb)
* Official [ESPnet](https://github.com/espnet/espnet) English/Chinese/Japanese TTS notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)
* Official [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron) LJSpeech TTS notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/as-ideas/ForwardTacotron/blob/master/notebooks/synthesize.ipynb)

## Speech Recognition
* [mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech) with LM on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/MozillaDeepSpeech.ipynb)
* Wav2Letter+ from [NVIDIA/OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq.git) without LM on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaWav2LetterPlus.ipynb)
* Jasper from [NVIDIA/OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq.git) without LM on Youtube videos. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaJasper.ipynb)
* QuartzNet from [NVIDIA/Nemo](https://github.com/NVIDIA/NeMo.git) without LM on Youtube videos. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaQuartzNet.ipynb)
* QuartzNet from [NVIDIA/Nemo](https://github.com/NVIDIA/NeMo.git) without LM with microphone. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaQuartzNetMic.ipynb)
* Official [ESPnet](https://github.com/espnet/espnet) Spanish->English speech translation notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)

## Object Detection
* Tensorflow object detection: FasterRCNN+InceptionResNet and ssd+mobilenet.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)
* Cascade RCNN demo using [open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Open_MMLab_Detection_Toolbox_Cascade_RCNN.ipynb)
* YOLO v3 demo using [ayooshkathuria/pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/YOLOv3_PyTorch.ipynb)
* YOLO v4 with [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE)
* YOLO v5 with [ultralytics/yolov5](https://github.com/ultralytics/yolov5). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb)
* Object detection on Youtube videos using [amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch) (SSD300).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SSD_Pytorch_Video.ipynb)  
* CenterNet (Objects as Points) demo using [xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CenterNet_ObjectsAsPoints.ipynb)
* Official DE⫶TR demo notebook [facebookresearch/detr](https://github.com/facebookresearch/detr).
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)
* Official Google [EfficientDet](https://arxiv.org/abs/1911.09070) notebook.
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/automl/blob/master/efficientdet/tutorial.ipynb)
* Test and train box-models from [Tensorflow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with [dctian/DeepPiCar](https://github.com/dctian/DeepPiCar). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dctian/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb)

## Segmentation
* Semantic segmentation trained on [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) using [CSAILVision/semantic-segmentation-pytorch](https://github.com/CSAILVision/semantic-segmentation-pytorch).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CSAILVision_SemanticSegmentation.ipynb)
* [DeepLabV3](https://arxiv.org/abs/1706.05587) from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionDeepLabV3.ipynb)
* Fast tracking and segmentation with [SiamMask](https://github.com/foolwood/SiamMask) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SiamMask.ipynb)
* Real-time semantic segmentation with [LightNet++](https://github.com/ansleliu/LightNetPlusPlus) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/LightNetPlusPlus.ipynb)
* Real-time instance segmentation with [YOLACT](https://github.com/dbolya/yolact) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/YOLACT.ipynb)
* Instance segmentation with [CenterMask](https://github.com/youngwanLEE/CenterMask/).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CenterMask.ipynb)
* Train and test [Tensorflow detection model zoo mask models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with [TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model). There is also [a tutorial](https://gilberttanner.com/blog/train-a-mask-r-cnn-model-with-the-tensorflow-object-detection-api) dedicated to this repo. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model/blob/master/Tensorflow_Object_Detection_API_Instance_Segmentation_in_Google_Colab.ipynb)
* Open source semantic segmentation toolbox [open-mmlab/mmsegmentation](https://github.com/open-mmlab/mmsegmentation). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb)
* Mask RCNN demo using [matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Matterport_Mask_RCNN.ipynb)
* Mask RCNN demo using [Detectron](https://github.com/facebookresearch/Detectron).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Detectron_MaskRCNN.ipynb)
* Detectron2:
  * Official Detectron2 Mask RCNN demo with [facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)
  * A combination of [facebookresearch/detectron2](https://github.com/facebookresearch/detectron2), [zhanghang1989/detectron2-ResNeSt](https://github.com/zhanghang1989/detectron2-ResNeSt) and [youngwanLEE/centermask2](https://github.com/youngwanLEE/centermask2) with my fork [styler00dollar/Colab-Detectron2](https://github.com/styler00dollar/Colab-Detectron2): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Detectron2-with-Original-and-ResNeSt/blob/resnest/Colab-Detectron2-(Original%2BResNeSt).ipynb)
* Mask RCNN demo from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionMaskRCNN.ipynb)
* Example usage of [open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection) with my fork [styler00dollar/Colab-mmdetection](https://github.com/styler00dollar/Colab-mmdetection). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-mmdetection/blob/master/Colab-mmdetection.)

## Multi Object Tracking
* Pedestrian tracking using [ZQPei/deep_sort_pytorch](https://github.com/ZQPei/deep_sort_pytorch) (DeepSORT + YOLOv3).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DeepSORT_YOLOv3.ipynb)  

## Pose Detection
* [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/OpenPose.ipynb)
* [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) on Youtube videos.
  * v0.2.0: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/AlphaPose.ipynb)
  * v0.3.0: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/AlphaPoseV0_3_0.ipynb)
* [DensePose](https://github.com/facebookresearch/DensePose) demo notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DensePose.ipynb)
* HRNet using [lxy5513/hrnet](https://github.com/lxy5513/hrnet) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/HRNet_lxy5513.ipynb)
* Keypoint R-CNN from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionPersonKeypoint.ipynb)

## Scene Text Detection
* [PixelLink](https://github.com/ZJULearning/pixel_link) demo notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/PixelLink.ipynb)
* Scene text detection using [argman/EAST](https://github.com/argman/EAST).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/EAST.ipynb)
* Scene text detection using [CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CRAFT.ipynb)

## Image generation
* Text-guided StyleGAN2 image generation with [orpatashnik/StyleCLIP](https://github.com/orpatashnik/StyleCLIP). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/optimization_playground.ipynb)
* Text-based image generation with SIREN and CLIP. Original notebook from [here](https://twitter.com/advadnoun/status/1348375026697834496?s=19). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP?usp=sharing)
  * A more compact version can be found inside [lucidrains/deep-daze](https://github.com/lucidrains/deep-daze). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing) 
  * My version with small modifications to [lucidrains/deep-daze](https://github.com/lucidrains/deep-daze). Less printing and Google Drive support with [styler00dollar/Colab-deep-daze](https://github.com/styler00dollar/Colab-deep-daze). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-deep-daze/blob/main/Colab-Deep-Daze.ipynb)
* Taming Transformers for High-Resolution Image Synthesis with [CompVis/taming-transformers](https://github.com/CompVis/taming-transformers). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb)
  * My fork with smaller Colab is located in [styler00dollar/Colab-taming-transformers](https://github.com/styler00dollar/Colab-taming-transformers).  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-taming-transformers/blob/master/Colab-taming-transformers.ipynb)
* BigGAN:
  * BigGAN [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)
  * Text-based image generation with BigGAN and CLIP:
    * Colab by [@advadnoun](https://twitter.com/advadnoun/status/1351038053033406468). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing#scrollTo=reChJi_vrNI_)
      * A modification of that notebook to reduce amount of text/clicks can be found inside [styler00dollar/Colab-BigGANxCLIP](https://github.com/styler00dollar/Colab-BigGANxCLIP). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-BigGANxCLIP/blob/main/Colab-BigGANxCLIP.ipynb)
      * Another modification of that Colab by nmkd: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Q2DIeMqYm_Sc5mlurnnurMMVqlgXpZNO?usp=sharing)
* Colab by [@eyaler](https://twitter.com/eyaler/status/1351044325392719876) / [eyaler/clip_biggan](https://github.com/eyaler/clip_biggan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/clip_biggan/blob/main/ClipBigGAN.ipynb)
    * Alternative Colab with CMA-ES is also inside [eyaler/clip_biggan](https://github.com/eyaler/clip_biggan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/clip_biggan/blob/main/WanderCLIP.ipynb)
* StyleGAN2:
  * StyleGAN2 with Differentiable Augmentation with [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/zsyzzsoft/5fbb71b9bf9a3217576bebae5de46fc2/data-efficient-gans.ipynb#scrollTo=Re5R6VX8VNgo) 
  * Style-based GAN architecture (StyleGAN2) can be found in [NVlabs/stylegan2](https://github.com/NVlabs/stylegan2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/parthsuresh/stylegan2-colab/blob/master/StyleGAN2_Google_Colab.ipynb)
  * Generating images from caption and vice versa via CLIP-Guided Generative Latent Space Search with [galatolofederico/clip-glass](https://github.com/galatolofederico/clip-glass). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fWka_U56NhCegbbrQPt4PWpHPtNRdU49?usp=sharing)
  * StyleGAN2 (ADA) with [eps696/stylegan2ada](https://github.com/eps696/stylegan2ada). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eps696/stylegan2ada/blob/master/StyleGAN2a_colab.ipynb#scrollTo=UOToqrX68APO)
  * Create StyleGAN2 Steam banners with [woctezuma/steam-stylegan2](https://github.com/woctezuma/steam-stylegan2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/woctezuma/steam-stylegan2/blob/master/StyleGAN2_image_sampling.ipynb#scrollTo=C_PIEdzNRADH)
  * StyleGAN2-ada with [eps696/stylegan2ada](https://github.com/eps696/stylegan2ada). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eps696/stylegan2ada/blob/master/StyleGAN2a_colab.ipynb)
  * StyleGAN2 with [eps696/stylegan2](https://github.com/eps696/stylegan2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eps696/stylegan2/blob/master/StyleGAN2_colab.ipynb)
* Anime+StyleGAN2:
    * Style-based GAN architecture (StyleGAN2) with anime face generation. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Pv8OIFlonha4KeYyY2oEFaK4mG-alaWF#scrollTo=q8VnyjDhiBQY&forceEdit=true&sandboxMode=true)
    * [Newest model training attempt by aydao (@AydaoAI) and Colab provided by arfa (@arfafax)](https://twitter.com/arfafax/status/1351604423303127040). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1oxcJ1tbG77hlggdKd_d8h22nBcIZsLTL?usp=sharing)
      * That combined with CLIP provided by [nagolinc/notebooks](https://github.com/nagolinc/notebooks). **(Warning: Results usually look bad, even had code related errors when I tested it.)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb)
      * A more compact and fixed version of that notebook by me. (Only fixed syntax errors, results still not reliable.) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17d4kfpqCJlbpuiNh4h-edyMfTfn0Hc-N?usp=sharing)
      * A V2 appeared inside the original repo [nagolinc/notebooks](https://github.com/nagolinc/notebooks), but still has quite a lot of boxes. The ``psi`` parameter is also gone. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/CLIP_%2B_TADNE_(pytorch)_v2.ipynb)
* Closed-Form Factorization of Latent Semantics in GANs with [genforce/sefa](https://github.com/genforce/sefa). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/genforce/sefa/blob/master/docs/SeFa.ipynb)
* Using the VAE from [openai/DALL-E](https://github.com/openai/DALL-E) and combining it with [openai/CLIP](https://github.com/openai/CLIP). [Colab by @advadnoun](https://twitter.com/advadnoun/status/1364822183751471109). (Warning: This is not real DALLE-E, it just uses the official VAE) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Q-TbYvASMPRMXCOQjkxxf72CXYjR_8Vp?usp=sharing#scrollTo=EbCH111HrTjw)
* CLIP + FFT with [eps696/aphantasia](https://github.com/eps696/aphantasia). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eps696/aphantasia/blob/master/Aphantasia.ipynb)
* VQGAN + CLIP by [Katherine Crowson](https://github.com/crowsonkb). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/15UwYDsnNeldJFHJ9NdgYBYeo6xPmSelP#scrollTo=wSfISAhyPmyp)

## Image colorization
* [DeOldify](https://github.com/jantic/DeOldify): A Deep Learning based project for colorizing and restoring old images.
  * My fork that combines deoldify anime and normal deoldify is located in [styler00dollar/Colab-DeOldify](https://github.com/styler00dollar/Colab-DeOldify). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeOldify/blob/master/Colab-DeOldify-(Original%2BAnime).ipynb)
  * deoldify an image (artistic model) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)
  * deoldify an image (stable model) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ColorizeTrainingStable.ipynb)
  * deoldify an anime image with [Dakini/AnimeColorDeOldify](https://github.com/Dakini/AnimeColorDeOldify). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Dakini/AnimeColorDeOldify/blob/master/ImageColorizerColab.ipynb)
* Coloring images with [pvitoria/ChromaGAN](https://github.com/pvitoria/ChromaGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pvitoria/ChromaGAN/blob/master/DemoChromaGAN.ipynb)
* Anime image colorization with reference image. [delta6189/Anime-Sketch-Colorizer](https://github.com/delta6189/Anime-Sketch-Colorizer) contains notebooks, but I created a version that does work with Colab which can be found inside [styler00dollar/Colab-Anime-Sketch-Colorizer](https://github.com/styler00dollar/Colab-Anime-Sketch-Colorizer). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Anime-Sketch-Colorizer/blob/master/Colab-Anime-Sketch-Colorizer.ipynb)
* A lightweight genative architexture for image inpainting wih [GuardSkill/
ptiveGAN](https://github.com/GuardSkill/AdaptiveGAN). My fork is located in [styler00dollar/Colab-AdaptiveGAN](https://github.com/styler00dollar/Colab-AdaptiveGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-AdaptiveGAN/blob/master/Colab-AdaptiveGAN.ipynb)
* Instance-aware Image Colorization with [ericsujw/InstColorization](https://github.com/ericsujw/InstColorization). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb)

## Image modification
* Discovering Interpretable GAN Controls with [harskish/ganspace](https://github.com/harskish/ganspace). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harskish/ganspace/blob/master/notebooks/Ganspace_colab.ipynb)
* Image to Sketch with [vijishmadhavan/ArtLine](https://github.com/vijishmadhavan/ArtLine).
  * Smooth: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/ArtLine.ipynb)
  * Quality: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/ArtLine(AR).ipynb)
* Animefy an image with StyleGAN2 and [XingruiWang/Animefy](https://github.com/XingruiWang/Animefy). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/XingruiWang/Animefy/blob/master/Notebook/Animefy-yourself-new.ipynb)
* Text-guided StyleGAN2 image modification with [orpatashnik/StyleCLIP](https://github.com/orpatashnik/StyleCLIP). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/StyleCLIP_global.ipynb)
* Restore old photos with [microsoft/Bringing-Old-Photos-Back-to-Life](https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing)
* Decensoring Hentai with Deep Neural Networks. The original repo is [deeppomf/DeepCreamPy](https://github.com/deeppomf/DeepCreamPy) and my fork is located in [styler00dollar/Colab-DeepCreamPy](https://github.com/styler00dollar/Colab-DeepCreamPy). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepCreamPy/blob/master/Colab-DeepCreamPy-2.0.ipynb)
* [dreamnettech/dreampower](https://github.com/dreamnettech/dreampower) is a deep learning algorithm based on DeepNude with the ability to nudify photos of people. My fork is located in [styler00dollar/Colab-dreampower](https://github.com/styler00dollar/Colab-dreampower). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-dreampower/blob/master/Colab-Dream.ipynb)
* Deblurring pictures with [TAMU-VITA/DeblurGANv2](https://github.com/TAMU-VITA/DeblurGANv2). My own fork is located in [styler00dollar/Colab-DeblurGANv2](https://github.com/styler00dollar/Colab-DeblurGANv2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeblurGANv2/blob/master/Colab-DeblurGANv2.ipynb)
* Deblurring pictures with [SeungjunNah/DeepDeblur-PyTorch](https://github.com/SeungjunNah/DeepDeblur-PyTorch). My own fork is located in [styler00dollar/Colab-DeepDeblur](https://github.com/styler00dollar/Colab-DeepDeblur). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepDeblur/blob/master/Colab-DeepDeblur.ipynb)
* [gordicaleksa/pytorch-deepdream](https://github.com/gordicaleksa/pytorch-deepdream) will give you the power to create weird and psychedelic-looking images. My fork is located in [styler00dollar/Colab-deepdream](https://github.com/styler00dollar/Colab-deepdream). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-deepdream/blob/master/Colab-deepdream.ipynb)
* Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline with [alex04072000/SingleHDR](https://github.com/alex04072000/SingleHDR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WzNaGSaucF2AMDSdUCBMEOauBg4IowMa#scrollTo=Q_gXemVL-1Zt)
* Watermark removal with [vinthony/deep-blind-watermark-removal](https://github.com/vinthony/deep-blind-watermark-removal). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pYY7byBjM-7aFIWk8HcF9nK_s6pqGwww?usp=sharing)
  * A more userfriendly version that does allow custom input with my fork [styler00dollar/Colab-deep-watermark](https://github.com/styler00dollar/Colab-deep-watermark). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-deep-watermark/blob/main/Colab-deep-watermark.ipynb)
* Versatile Image-to-Image Translation with [linjx-ustc1106/TuiGAN-PyTorch](https://github.com/linjx-ustc1106/TuiGAN-PyTorch). My Colab is located in [styler00dollar/Colab-TuiGAN](https://github.com/styler00dollar/Colab-TuiGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-TuiGAN/blob/master/Colab-TuiGAN.ipynb)
* Image denoising with [cszn/DPIR](https://github.com/cszn/DPIR). My colab is located in [styler00dollar/Colab-DPIR](https://github.com/styler00dollar/Colab-DPIR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DPIR/blob/master/Colab-DPIR.ipynb)
* Blind Face Restoration via Deep Multi-scale Component Dictionaries with [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tg-bomze/DFDNet/blob/whole/DFDNet_Colab.ipynb)
  * Alternative Colab provided by [xinntao/BasicSR](https://github.com/xinntao/BasicSR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RoNDeipp9yPjI3EbpEbUhn66k5Uzg4n8)
* Training DFDnet with pytorch and pytorch lightning with [styler00dollar/Colab-DFDNet](https://github.com/styler00dollar/Colab-DFDNet). Uses the fork [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet) as a base, which offers more/better code than the official repo [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet).
  * Pytorch: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DFDNet/blob/master/Colab-DFDNet.ipynb)
  * Pytorch lightning (more loss, cleaner code, multi-gpu, etc.): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DFDNet/blob/master/Colab-DFDNet-lightning-train.ipynb)
* Style transfer with [linjx-ustc1106/TuiGAN-PyTorch](https://github.com/linjx-ustc1106/TuiGAN-PyTorch) and my fork [styler00dollar/Colab-TuiGAN](https://github.com/styler00dollar/Colab-TuiGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-TuiGAN/blob/master/Colab-TuiGAN.ipynb)
* Face editing with [mit-han-lab/anycost-gan](https://github.com/mit-han-lab/anycost-gan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb)
* Remove background with [PeterL1n/BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2).
  * Image: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cTxFq1YuoJ5QPqaTcnskwlHDolnjBkB9?usp=sharing)
* Modify attributes of anime faces like eyes, mouth, rotation, etc. with [pkhungurn/talking-head-anime-2-demo](https://github.com/pkhungurn/talking-head-anime-2-demo). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pkhungurn/talking-head-anime-2-demo/blob/master/colab.ipynb)
* Reconstruct images and merge multiple images together with [chail/latent-composition](https://github.com/chail/latent-composition). My fork is located in [styler00dollar/Colab-latent-composition](https://github.com/styler00dollar/Colab-latent-composition). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-latent-composition/blob/main/Colab-latent-composition.ipynb)
* A Residual-Based StyleGAN Encoder via Iterative Refinement with [yuval-alaluf/restyle-encoder](https://github.com/yuval-alaluf/restyle-encoder). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yuval-alaluf/restyle-encoder/blob/master/notebooks/inference_playground.ipynb)

## Video modification
* A Google Colab notebook set up for both conventional and machine learning-based video processing. This repo combines VapourSynth and ESRGAN and is located in [AlphaAtlas/VapourSynthColab](https://github.com/AlphaAtlas/VapourSynthColab). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlphaAtlas/VapourSynthColab/blob/master/VapourSynthColab.ipynb)
* Generates a talking face video from an image and an audio using [Rudrabha/LipGAN](https://github.com/Rudrabha/LipGAN).
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/LipGAN.ipynb)
* Deoldify a video. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)
* Decensoring mosaic with [HypoX64/DeepMosaics](https://github.com/HypoX64/DeepMosaics). My own colab fork is located in [styler00dollar/Colab-DeepMosaics](https://github.com/styler00dollar/Colab-DeepMosaics). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepMosaics/blob/master/Colab-DeepMosaics.ipynb)
* Inpaint video with [vt-vl-lab/FGVC](https://github.com/vt-vl-lab/FGVC). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc?usp=sharing)
* Remove background with [PeterL1n/BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2).
  * Video: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Y9zWfULc8-DDTSsCH-pX6Utw8skiJG5s?usp=sharing)

## Image classification
* Image classification with [bentrevett/pytorch-image-classification](https://github.com/bentrevett/pytorch-image-classification). 
  * Multilayer Perceptron: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb)
  * LeNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb)
  * AlexNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb)
  * VGG: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/4_vgg.ipynb)
  * ResNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/5_resnet.ipynb)
  * My very compact version of ResNet by me with [styler00dollar/Colab-image-classification](https://github.com/styler00dollar/Colab-image-classification): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-image-classification/blob/master/5_(small)_ResNet.ipynb)
* More completely new classification notebooks that are within [styler00dollar/Colab-image-classification](https://github.com/styler00dollar/Colab-image-classification).
  * ResNeSt: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/6_ResNeSt.ipynb)
  * EfficientNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/7_EfficientNet.ipynb)
  * EfficientNet (lightning): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/7_EfficientNet_lightning.ipynb)
  * mobilenet_v3: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/8_mobilenet_v3.ipynb)
  * SqueezeNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/9_SqueezeNet.ipynb)
  * RepVGG: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/10_RepVGG.ipynb)
  * vit (lightning): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/styler00dollar/Colab-image-classification/blob/master/11_vit_lightning.ipynb)
* Using Autoencoders for unsupervised classification with [ardamavi/Unsupervised-Classification-with-Autoencoder](https://github.com/ardamavi/Unsupervised-Classification-with-Autoencoder). My fork [styler00dollar/Colab-UnsupervisedClassification](https://github.com/styler00dollar/Colab-UnsupervisedClassification) has some improvements and is usable with Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](styler00dollar/Colab-image-classification/blob/master/6_ResNeSt.ipynb)
* OpenMMLab Image Classification Toolbox and Benchmark [open-mmlab/mmclassification](https://github.com/open-mmlab/mmclassification). My fork is located in [styler00dollar/Colab-mmclassification](https://github.com/styler00dollar/Colab-mmclassification). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-mmclassification/blob/master/Colab-mmclassification.ipynb)
* Tokens-to-Token ViT: Classification of images with transformers and [yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT) and my fork [styler00dollar/Colab-T2T-ViT](https://github.com/styler00dollar/Colab-T2T-ViT). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-T2T-ViT/blob/main/Colab-T2T-ViT.ipynb)

## NLP
* Ai Dungeon alternative with [finetuneanon/gpt-neo_dungeon](https://github.com/finetuneanon/gpt-neo_dungeon). [**Warning: Because of high amount of Google Drive requrests to download the models, there can be errors when you try to use the Colab. It's recommended to either retry another time if you get errors or download the models with the torrent (magnet in Colab) and upload model to your own Drive.**] [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/finetuneanon/gpt-neo_dungeon/blob/master/gpt-neo_dungeon.ipynb)
* GPT2 English with [thecoder-001/GPT-2](https://github.com/thecoder-001/GPT-2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayush06feb/GPT-2/blob/master/GPT2.ipynb)
  * A small modification by me to fix usage: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WvcIRwiUlV2Wfoaca9O6Fi3ieW_qXujY?usp=sharing)
* Finetune GPT2 with [ak9250/gpt-2-colab](https://github.com/ak9250/gpt-2-colab/). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb)
* Using russian GPT2 and 3 (based on 2) with [sberbank-ai/ru-gpts](https://github.com/sberbank-ai/ru-gpts). 
  * Generation with ruGPT3large: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/ruGPT3_generation_example.ipynb#scrollTo=Y8DYR4d6_-w7)
  * Finetuning ruGPT3Small: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Finetune_ruGPT3Small.ipynb#scrollTo=5vL07XFvsBBU)

## Misc
* Defocus Blur Detection via Depth Distillation with [vinthony/depth-distillation](https://github.com/vinthony/depth-distillation). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1a-Un_lZqkEN-mr-SzQh9GLy4qXIJgn0v#scrollTo=Lh2_NGuLaM_c)
* Generate a human 3d model from a 2d picture with [facebookresearch/pifuhd](https://github.com/facebookresearch/pifuhd). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing)
* Turn a 2D image into a 3D video with [ai-coodinator/3D-Photo-Inpainting](https://github.com/ai-coodinator/3D-Photo-Inpainting) and [vt-vl-lab /3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai-coodinator/3D-Photo-Inpainting/blob/master/3D_Photo_Inpainting.ipynb)
* Using Mask-RCNN and ESRGAN to detect barsand mosaic and depixelate content with [natethegreate/hent-AI](https://github.com/natethegreate/hent-AI). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/natethegreate/hent-AI/blob/master/hent_AI_COLAB_1.ipynb)
* Music Source Separation [sigsep/open-unmix-pytorch](https://github.com/sigsep/open-unmix-pytorch). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ)
* First Order Motion Model for Image Animation [AliaksandrSiarohin/first-order-model](https://github.com/AliaksandrSiarohin/first-order-model). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)
* Official notebook of 3D Photography using Context-aware Layered Depth Inpainting [vt-vl-lab/3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)
* [Image-GPT](https://github.com/openai/image-gpt) notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Image_GPT_Sample_with_Conditioning.ipynb)
* Lifespan Age Transformation Synthesis with [royorel/Lifespan_Age_Transformation_Synthesis](https://github.com/royorel/Lifespan_Age_Transformation_Synthesis). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/royorel/Lifespan_Age_Transformation_Synthesis/blob/master/LATS_demo.ipynb#scrollTo=TSUbmd697Api)
* Bayesian Image Reconstruction (inpainting and super resolution) using Deep Generative Models with [razvanmarinescu/brgm](https://github.com/razvanmarinescu/brgm). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1G7_CGPHZVGFWIkHOAke4HFg06-tNHIZ4?usp=sharing#scrollTo=qMgE6QFiHuSL)
* Contrastive Language-Image Pre-Training with [openai/CLIP](https://github.com/openai/CLIP). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb#scrollTo=0BpdJkdBssk9)
* Real-time View Synthesis with [nex-mpi/nex-code](https://github.com/nex-mpi/nex-code). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hXVvYdAwLA0EFg2zrafJUE0bFgB_F7PU#scrollTo=TFbN4mrJCp8o&sandboxMode=true)
